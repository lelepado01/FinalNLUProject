{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHEyWj7DCx_c"
      },
      "source": [
        "# NLU Intent Classification & Slot Filling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnM6jPDg1gCe",
        "outputId": "42845169-a8b3-4236-d8a5-c5b632cbd300"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uRqithOv9Ym",
        "outputId": "3e08930f-5c49-4a06-fdaf-760e673c1cd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9pxBwKVPAd2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "device = 'cuda:0' # cuda:0 means we are using the GPU with id 0, if you have multiple GPU\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" # Used to report errors on CUDA side\n",
        "PAD_TOKEN = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJztmYRcPiZM"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from pprint import pprint\n",
        "import random\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCO6OoUOCsc3"
      },
      "source": [
        "# Data Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZbFHOOUqMtp"
      },
      "outputs": [],
      "source": [
        "def load_data(path):\n",
        "    '''\n",
        "        input: path/to/data\n",
        "        output: json \n",
        "    '''\n",
        "    dataset = []\n",
        "    with open(path) as f:\n",
        "        dataset = json.loads(f.read())\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lB0jl7Z4Rcth"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "class Lang():\n",
        "    def __init__(self, words, intents, slots, cutoff=0):\n",
        "        self.word2id = self.w2id(words, cutoff=cutoff, unk=True)\n",
        "        self.slot2id = self.lab2id(slots)\n",
        "        self.intent2id = self.lab2id(intents, pad=False)\n",
        "        self.id2word = {v:k for k, v in self.word2id.items()}\n",
        "        self.id2slot = {v:k for k, v in self.slot2id.items()}\n",
        "        self.id2intent = {v:k for k, v in self.intent2id.items()}\n",
        "        \n",
        "    def w2id(self, elements, cutoff=None, unk=True):\n",
        "        vocab = {'pad': PAD_TOKEN}\n",
        "        if unk:\n",
        "            vocab['unk'] = len(vocab)\n",
        "        count = Counter(elements)\n",
        "        for k, v in count.items():\n",
        "            if v > cutoff:\n",
        "                vocab[k] = len(vocab)\n",
        "        return vocab\n",
        "    \n",
        "    def lab2id(self, elements, pad=True):\n",
        "        vocab = {}\n",
        "        if pad:\n",
        "            vocab['pad'] = PAD_TOKEN\n",
        "        for elem in elements:\n",
        "                vocab[elem] = len(vocab)\n",
        "        return vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNAkF9uRRjx9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.utils.data as data\n",
        "\n",
        "class IntentsAndSlots (data.Dataset):\n",
        "    # Mandatory methods are __init__, __len__ and __getitem__\n",
        "    def __init__(self, dataset, lang, unk='unk'):\n",
        "        self.utterances = []\n",
        "        self.intents = []\n",
        "        self.slots = []\n",
        "        self.unk = unk\n",
        "        \n",
        "        for x in dataset:\n",
        "            self.utterances.append(x['utterance'])\n",
        "            self.slots.append(x['slots'])\n",
        "            self.intents.append(x['intent'])\n",
        "\n",
        "        self.utt_ids = self.mapping_seq(self.utterances, lang.word2id)\n",
        "        self.slot_ids = self.mapping_seq(self.slots, lang.slot2id)\n",
        "        self.intent_ids = self.mapping_lab(self.intents, lang.intent2id)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.utterances)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        utt = torch.Tensor(self.utt_ids[idx])\n",
        "        slots = torch.Tensor(self.slot_ids[idx])\n",
        "        intent = self.intent_ids[idx]\n",
        "        sample = {'utterance': utt, 'slots': slots, 'intent': intent}\n",
        "        return sample\n",
        "    \n",
        "    # Auxiliary methods\n",
        "    def mapping_lab(self, data, mapper):\n",
        "        return [mapper[x] if x in mapper else mapper[self.unk] for x in data]\n",
        "    \n",
        "    def mapping_seq(self, data, mapper): # Map sequences to number\n",
        "        res = []\n",
        "        for seq in data:\n",
        "            tmp_seq = []\n",
        "            for x in seq.split():\n",
        "                if x in mapper:\n",
        "                    tmp_seq.append(mapper[x])\n",
        "                else:\n",
        "                    tmp_seq.append(mapper[self.unk])\n",
        "            res.append(tmp_seq)\n",
        "        return res\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9du3GJ4Rs8W"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def collate_fn(data):\n",
        "    def merge(sequences):\n",
        "        '''\n",
        "        merge from batch * sent_len to batch * max_len \n",
        "        '''\n",
        "        lengths = [len(seq) for seq in sequences]\n",
        "        max_len = 1 if max(lengths)==0 else max(lengths)\n",
        "        # Pad token is zero in our case\n",
        "        # So we create a matrix full of PAD_TOKEN (i.e. 0) with the shape \n",
        "        # batch_size X maximum length of a sequence\n",
        "        padded_seqs = torch.LongTensor(len(sequences),max_len).fill_(PAD_TOKEN)\n",
        "        for i, seq in enumerate(sequences):\n",
        "            end = lengths[i]\n",
        "            padded_seqs[i, :end] = seq # We copy each sequence into the matrix\n",
        "        # print(padded_seqs)\n",
        "        padded_seqs = padded_seqs.detach()  # We remove these tensors from the computational graph\n",
        "        return padded_seqs, lengths\n",
        "    # Sort data by seq lengths\n",
        "    data.sort(key=lambda x: len(x['utterance']), reverse=True) \n",
        "    new_item = {}\n",
        "    for key in data[0].keys():\n",
        "        new_item[key] = [d[key] for d in data]\n",
        "    # We just need one length for packed pad seq, since len(utt) == len(slots)\n",
        "    src_utt, _ = merge(new_item['utterance'])\n",
        "    y_slots, y_lengths = merge(new_item[\"slots\"])\n",
        "    intent = torch.LongTensor(new_item[\"intent\"])\n",
        "    \n",
        "    src_utt = src_utt#.to(device) # We load the Tensor on our seleceted device\n",
        "    y_slots = y_slots#.to(device)\n",
        "    intent = intent#.to(device)\n",
        "    y_lengths = torch.LongTensor(y_lengths)#.to(device)\n",
        "    \n",
        "    new_item[\"utterances\"] = src_utt\n",
        "    new_item[\"intents\"] = intent\n",
        "    new_item[\"y_slots\"] = y_slots\n",
        "    new_item[\"slots_len\"] = y_lengths\n",
        "    return new_item\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfSVj9KsqZpj"
      },
      "outputs": [],
      "source": [
        "def get_lang(train_raw, dev_raw, test_raw):\n",
        "    words = sum([x['utterance'].split() for x in train_raw], []) # No set() since we want to compute the cutoff\n",
        "    corpus = train_raw + dev_raw + test_raw # We do not wat unk labels, \n",
        "                                            # however this depends on the research purpose\n",
        "    slots = set(sum([line['slots'].split() for line in corpus],[]))\n",
        "    intents = set([line['intent'] for line in corpus])\n",
        "\n",
        "    return Lang(words, intents, slots, cutoff=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0OpLuhznjpR"
      },
      "source": [
        "# Data Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load raw data from json files, both for test and train set.\n",
        "\n",
        "The SNIPS dataset also contains a dedicated file for validation set, while this has to be obtained for ATIS, from a subset of the train set."
      ],
      "metadata": {
        "id": "IH4lJS21mQ0J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OP-FThM7n3D6"
      },
      "outputs": [],
      "source": [
        "tmp_train_raw_ATIS = load_data(os.path.join('./drive/MyDrive/data/','atis','train.json'))\n",
        "test_raw_ATIS = load_data(os.path.join('./drive/MyDrive/data/','atis','test.json'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-BIXZjEoZ_x"
      },
      "outputs": [],
      "source": [
        "train_raw_SNIPS   = load_data(os.path.join('./drive/MyDrive/data/','snips','train.json'))\n",
        "test_raw_SNIPS = load_data(os.path.join('./drive/MyDrive/data/','snips','test.json'))\n",
        "dev_raw_SNIPS = load_data(os.path.join('./drive/MyDrive/data/','snips','valid.json'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hToFRUlo-7L"
      },
      "outputs": [],
      "source": [
        "# Firt we get the 10% of dataset, then we compute the percentage of these examples \n",
        "# on the training set which is around 11% \n",
        "portion = round(((len(tmp_train_raw_ATIS) + len(test_raw_ATIS)) * 0.10)/(len(tmp_train_raw_ATIS)),2)\n",
        "\n",
        "\n",
        "intents_ATIS = [x['intent'] for x in tmp_train_raw_ATIS] # We stratify on intents\n",
        "count_y_ATIS = Counter(intents_ATIS)\n",
        "\n",
        "Y_ATIS = []\n",
        "X_ATIS = []\n",
        "mini_Train_ATIS = []\n",
        "\n",
        "for id_y, y in enumerate(intents_ATIS):\n",
        "    # IT MAKES NO SENSE TO PUT IN DEV SET INTENTS THAT ONLY APPEAR ONCE\n",
        "    if count_y_ATIS[y] > 1: # Some intents have only one instance, we put them in training\n",
        "        X_ATIS.append(tmp_train_raw_ATIS[id_y])\n",
        "        Y_ATIS.append(y)\n",
        "    else:\n",
        "        mini_Train_ATIS.append(tmp_train_raw_ATIS[id_y])\n",
        "# Random Stratify\n",
        "X_train_ATIS, X_dev_ATIS, y_train_ATIS, y_dev_ATIS = train_test_split(X_ATIS, Y_ATIS, test_size=portion, \n",
        "                                                    random_state=42, \n",
        "                                                    shuffle=True,\n",
        "                                                    stratify=Y_ATIS)\n",
        "X_train_ATIS.extend(mini_Train_ATIS)\n",
        "train_raw_ATIS = X_train_ATIS\n",
        "dev_raw_ATIS = X_dev_ATIS\n",
        "\n",
        "y_test_ATIS = [x['intent'] for x in test_raw_ATIS]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMDIu6o0qRbh"
      },
      "outputs": [],
      "source": [
        "lang_ATIS = get_lang(train_raw_ATIS, dev_raw_ATIS, test_raw_ATIS)\n",
        "lang_SNIPS = get_lang(train_raw_SNIPS, dev_raw_SNIPS, test_raw_SNIPS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEu7GY_KRl8s"
      },
      "outputs": [],
      "source": [
        "train_dataset_ATIS = IntentsAndSlots(train_raw_ATIS, lang_ATIS)\n",
        "dev_dataset_ATIS = IntentsAndSlots(dev_raw_ATIS, lang_ATIS)\n",
        "test_dataset_ATIS = IntentsAndSlots(test_raw_ATIS, lang_ATIS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2NlChTUpiyD"
      },
      "outputs": [],
      "source": [
        "train_dataset_SNIPS = IntentsAndSlots(train_raw_SNIPS, lang_SNIPS)\n",
        "dev_dataset_SNIPS = IntentsAndSlots(dev_raw_SNIPS, lang_SNIPS)\n",
        "test_dataset_SNIPS = IntentsAndSlots(test_raw_SNIPS, lang_SNIPS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHFcx5IVppVk"
      },
      "outputs": [],
      "source": [
        "# Dataloader instantiation\n",
        "train_loader_ATIS = DataLoader(train_dataset_ATIS, batch_size=128, collate_fn=collate_fn,  shuffle=True)\n",
        "dev_loader_ATIS = DataLoader(dev_dataset_ATIS, batch_size=64, collate_fn=collate_fn)\n",
        "test_loader_ATIS = DataLoader(test_dataset_ATIS, batch_size=64, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRo3ToCrno7l"
      },
      "outputs": [],
      "source": [
        "# Dataloader instantiation\n",
        "train_loader_SNIPS = DataLoader(train_dataset_SNIPS, batch_size=128, collate_fn=collate_fn,  shuffle=True)\n",
        "dev_loader_SNIPS = DataLoader(dev_dataset_SNIPS, batch_size=64, collate_fn=collate_fn)\n",
        "test_loader_SNIPS = DataLoader(test_dataset_SNIPS, batch_size=64, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mGyFJIXg85d"
      },
      "outputs": [],
      "source": [
        "out_slot_ATIS = len(lang_ATIS.slot2id)\n",
        "out_int_ATIS = len(lang_ATIS.intent2id)\n",
        "vocab_len_ATIS = len(lang_ATIS.word2id)\n",
        "\n",
        "out_slot_SNIPS = len(lang_SNIPS.slot2id)\n",
        "out_int_SNIPS = len(lang_SNIPS.intent2id)\n",
        "vocab_len_SNIPS = len(lang_SNIPS.word2id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Gqu80ZnSfeg"
      },
      "source": [
        "# Datasets Stats"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To describe each dataset in the report, the number of samples is computed, as well as the first item of the train set is printed "
      ],
      "metadata": {
        "id": "7ij9SMCjnS6v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCEuy0-LSel5"
      },
      "outputs": [],
      "source": [
        "def get_dataset_stats(train, test, valid=None):\n",
        "    print('Train samples:', len(train))\n",
        "    if valid is not None: \n",
        "      print('Validation samples:', len(valid))\n",
        "    print('Test samples:', len(test))\n",
        "    pprint(train[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCFPfRpwTUJP",
        "outputId": "cc7ac253-b286-4bc7-9cfb-37f0eac0d3b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train samples: 4381\n",
            "Validation samples: 597\n",
            "Test samples: 893\n",
            "{'intent': 'aircraft',\n",
            " 'slots': 'O O O O O B-airline_name O O B-fromloc.city_name O '\n",
            "          'B-toloc.city_name B-depart_time.time_relative B-depart_time.time '\n",
            "          'I-depart_time.time',\n",
            " 'utterance': 'what type of aircraft does eastern fly from atlanta to denver '\n",
            "              'before 6 pm'}\n"
          ]
        }
      ],
      "source": [
        "get_dataset_stats(train_raw_ATIS, test_raw_ATIS, dev_raw_ATIS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPLmOptLTcYt",
        "outputId": "6f3e9137-daa3-417a-d1d2-600ab1102fbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train samples: 13084\n",
            "Validation samples: 700\n",
            "Test samples: 700\n",
            "{'intent': 'PlayMusic',\n",
            " 'slots': 'O O B-artist O B-album O B-service I-service',\n",
            " 'utterance': 'listen to westbam alumb allergic on google music'}\n"
          ]
        }
      ],
      "source": [
        "get_dataset_stats(train_raw_SNIPS, test_raw_SNIPS, dev_raw_SNIPS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfxYB_yRCQj2"
      },
      "source": [
        "# Conll.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mD4a6sxFSChf"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "\"\"\"\n",
        "Modified version of https://pypi.org/project/conlleval/\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def stats():\n",
        "    return {'cor': 0, 'hyp': 0, 'ref': 0}\n",
        "\n",
        "\n",
        "def evaluate(ref, hyp, otag='O'):\n",
        "    # evaluation for NLTK\n",
        "\t#print(len(ref), len(hyp))\n",
        "\taligned = align_hyp(ref, hyp)\n",
        "\treturn conlleval(aligned, otag=otag)\n",
        "\n",
        "\n",
        "def align_hyp(ref, hyp):\n",
        "    # align references and hypotheses for evaluation\n",
        "    # add last element of token tuple in hyp to ref\n",
        "    if len(ref) != len(hyp):\n",
        "        raise ValueError(\"Size Mismatch: ref: {} & hyp: {}\".format(len(ref), len(hyp)))\n",
        "\n",
        "    out = []\n",
        "    for i in range(len(ref)):\n",
        "        if len(ref[i]) != len(hyp[i]):\n",
        "            raise ValueError(\"Size Mismatch: ref: {} & hyp: {}\".format(len(ref), len(hyp)))\n",
        "        out.append([(*ref[i][j], hyp[i][j][-1]) for j in range(len(ref[i]))])\n",
        "    return out\n",
        "\n",
        "\n",
        "def conlleval(data, otag='O'):\n",
        "    # token, segment & class level counts for TP, TP+FP, TP+FN\n",
        "    tok = stats()\n",
        "    seg = stats()\n",
        "    cls = {}\n",
        "\n",
        "    for sent in data:\n",
        "\n",
        "        prev_ref = otag      # previous reference label\n",
        "        prev_hyp = otag      # previous hypothesis label\n",
        "        prev_ref_iob = None  # previous reference label IOB\n",
        "        prev_hyp_iob = None  # previous hypothesis label IOB\n",
        "\n",
        "        in_correct = False  # currently processed chunks is correct until now\n",
        "\n",
        "        for token in sent:\n",
        "\n",
        "            hyp_iob, hyp = parse_iob(token[-1])\n",
        "            ref_iob, ref = parse_iob(token[-2])\n",
        "\n",
        "            ref_e = is_eoc(ref, ref_iob, prev_ref, prev_ref_iob, otag)\n",
        "            hyp_e = is_eoc(hyp, hyp_iob, prev_hyp, prev_hyp_iob, otag)\n",
        "\n",
        "            ref_b = is_boc(ref, ref_iob, prev_ref, prev_ref_iob, otag)\n",
        "            hyp_b = is_boc(hyp, hyp_iob, prev_hyp, prev_hyp_iob, otag)\n",
        "\n",
        "            if not cls.get(ref) and ref:\n",
        "                cls[ref] = stats()\n",
        "\n",
        "            if not cls.get(hyp) and hyp:\n",
        "                cls[hyp] = stats()\n",
        "\n",
        "            # segment-level counts\n",
        "            if in_correct:\n",
        "                if ref_e and hyp_e and prev_hyp == prev_ref:\n",
        "                    in_correct = False\n",
        "                    seg['cor'] += 1\n",
        "                    cls[prev_ref]['cor'] += 1\n",
        "\n",
        "                elif ref_e != hyp_e or hyp != ref:\n",
        "                    in_correct = False\n",
        "\n",
        "            if ref_b and hyp_b and hyp == ref:\n",
        "                in_correct = True\n",
        "\n",
        "            if ref_b:\n",
        "                seg['ref'] += 1\n",
        "                cls[ref]['ref'] += 1\n",
        "\n",
        "            if hyp_b:\n",
        "                seg['hyp'] += 1\n",
        "                cls[hyp]['hyp'] += 1\n",
        "\n",
        "            # token-level counts\n",
        "            if ref == hyp and ref_iob == hyp_iob:\n",
        "                tok['cor'] += 1\n",
        "\n",
        "            tok['ref'] += 1\n",
        "\n",
        "            prev_ref = ref\n",
        "            prev_hyp = hyp\n",
        "            prev_ref_iob = ref_iob\n",
        "            prev_hyp_iob = hyp_iob\n",
        "\n",
        "        if in_correct:\n",
        "            seg['cor'] += 1\n",
        "            cls[prev_ref]['cor'] += 1\n",
        "\n",
        "    return summarize(seg, cls)\n",
        "\n",
        "\n",
        "def parse_iob(t):\n",
        "    m = re.match(r'^([^-]*)-(.*)$', t)\n",
        "    return m.groups() if m else (t, None)\n",
        "\n",
        "\n",
        "def is_boc(lbl, iob, prev_lbl, prev_iob, otag='O'):\n",
        "    \"\"\"\n",
        "    is beginning of a chunk\n",
        "    supports: IOB, IOBE, BILOU schemes\n",
        "        - {E,L} --> last\n",
        "        - {S,U} --> unit\n",
        "    :param lbl: current label\n",
        "    :param iob: current iob\n",
        "    :param prev_lbl: previous label\n",
        "    :param prev_iob: previous iob\n",
        "    :param otag: out-of-chunk label\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    boc = False\n",
        "\n",
        "    boc = True if iob in ['B', 'S', 'U'] else boc\n",
        "    boc = True if iob in ['E', 'L'] and prev_iob in ['E', 'L', 'S', otag] else boc\n",
        "    boc = True if iob == 'I' and prev_iob in ['S', 'L', 'E', otag] else boc\n",
        "\n",
        "    boc = True if lbl != prev_lbl and iob != otag and iob != '.' else boc\n",
        "\n",
        "    # these chunks are assumed to have length 1\n",
        "    boc = True if iob in ['[', ']'] else boc\n",
        "\n",
        "    return boc\n",
        "\n",
        "\n",
        "def is_eoc(lbl, iob, prev_lbl, prev_iob, otag='O'):\n",
        "    \"\"\"\n",
        "    is end of a chunk\n",
        "    supports: IOB, IOBE, BILOU schemes\n",
        "        - {E,L} --> last\n",
        "        - {S,U} --> unit\n",
        "    :param lbl: current label\n",
        "    :param iob: current iob\n",
        "    :param prev_lbl: previous label\n",
        "    :param prev_iob: previous iob\n",
        "    :param otag: out-of-chunk label\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    eoc = False\n",
        "\n",
        "    eoc = True if iob in ['E', 'L', 'S', 'U'] else eoc\n",
        "    eoc = True if iob == 'B' and prev_iob in ['B', 'I'] else eoc\n",
        "    eoc = True if iob in ['S', 'U'] and prev_iob in ['B', 'I'] else eoc\n",
        "\n",
        "    eoc = True if iob == otag and prev_iob in ['B', 'I'] else eoc\n",
        "\n",
        "    eoc = True if lbl != prev_lbl and iob != otag and prev_iob != '.' else eoc\n",
        "\n",
        "    # these chunks are assumed to have length 1\n",
        "    eoc = True if iob in ['[', ']'] else eoc\n",
        "\n",
        "    return eoc\n",
        "\n",
        "\n",
        "def score(cor_cnt, hyp_cnt, ref_cnt):\n",
        "    # precision\n",
        "    p = 1 if hyp_cnt == 0 else cor_cnt / hyp_cnt\n",
        "    # recall\n",
        "    r = 0 if ref_cnt == 0 else cor_cnt / ref_cnt\n",
        "    # f-measure (f1)\n",
        "    f = 0 if p+r == 0 else (2*p*r)/(p+r)\n",
        "    return {\"p\": p, \"r\": r, \"f\": f, \"s\": ref_cnt}\n",
        "\n",
        "\n",
        "def summarize(seg, cls):\n",
        "    # class-level\n",
        "    res = {lbl: score(cls[lbl]['cor'], cls[lbl]['hyp'], cls[lbl]['ref']) for lbl in set(cls.keys())}\n",
        "    # micro\n",
        "    res.update({\"total\": score(seg.get('cor', 0), seg.get('hyp', 0), seg.get('ref', 0))})\n",
        "    return res\n",
        "\n",
        "\n",
        "def read_corpus_conll(corpus_file, fs=\"\\t\"):\n",
        "    \"\"\"\n",
        "    read corpus in CoNLL format\n",
        "    :param corpus_file: corpus in conll format\n",
        "    :param fs: field separator\n",
        "    :return: corpus\n",
        "    \"\"\"\n",
        "    featn = None  # number of features for consistency check\n",
        "    sents = []  # list to hold words list sequences\n",
        "    words = []  # list to hold feature tuples\n",
        "\n",
        "    for line in open(corpus_file):\n",
        "        line = line.strip()\n",
        "        if len(line.strip()) > 0:\n",
        "            feats = tuple(line.strip().split(fs))\n",
        "            if not featn:\n",
        "                featn = len(feats)\n",
        "            elif featn != len(feats) and len(feats) != 0:\n",
        "                raise ValueError(\"Unexpected number of columns {} ({})\".format(len(feats), featn))\n",
        "\n",
        "            words.append(feats)\n",
        "        else:\n",
        "            if len(words) > 0:\n",
        "                sents.append(words)\n",
        "                words = []\n",
        "    return sents\n",
        "\n",
        "\n",
        "def get_chunks(corpus_file, fs=\"\\t\", otag=\"O\"):\n",
        "    sents = read_corpus_conll(corpus_file, fs=fs)\n",
        "    return set([parse_iob(token[-1])[1] for sent in sents for token in sent if token[-1] != otag])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IO83hCbAKGiO"
      },
      "source": [
        "# Plotting Losses and Confusion Matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To plot the training and validation loss, as well as the confusion matrices for the test phase, the two following auxiliary functions are used."
      ],
      "metadata": {
        "id": "lusRgPrsoLaj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2P09JuzKGBe"
      },
      "outputs": [],
      "source": [
        "def plot_losses(epochs, losses_train, losses_dev):\n",
        "    plt.figure(num = 3, figsize=(8, 5)).patch.set_facecolor('white')\n",
        "    plt.title('Train and Dev Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.plot(epochs, losses_train, label='Train Loss')\n",
        "    plt.plot(epochs, losses_dev, label='Dev Loss')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X14SC_6w8kfe"
      },
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "\n",
        "def plot_confusion_matrix(data, label=\"Confusion Matrix\"):\n",
        "    plt.figure(figsize = (10,10))\n",
        "    plt.title(label)\n",
        "    plt.imshow(data, norm = matplotlib.colors.Normalize(vmin = 0, vmax = 255))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFUz-wL9-sxi"
      },
      "source": [
        "# Early Stopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOTdG5yy-x8K"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping(): \n",
        "    def __init__(self, max_patience): \n",
        "        self.max_patience = max_patience \n",
        "        self.patience = max_patience\n",
        "        self.best = 0\n",
        "\n",
        "    def reset(self): \n",
        "        self.patience = self.max_patience\n",
        "        self.best = 0\n",
        "\n",
        "    def update(self, value): \n",
        "        if value > self.best:\n",
        "            self.best = value\n",
        "        else:\n",
        "            self.patience -= 1\n",
        "        \n",
        "        if self.patience <= 0:\n",
        "            return False \n",
        "\n",
        "        return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sfLELAWR-2j"
      },
      "source": [
        "# Train and Test Loops"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple train and test loop taken from the baseline code for the assigment "
      ],
      "metadata": {
        "id": "jQl-KNpzofdp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8c5qwuAR9kb"
      },
      "outputs": [],
      "source": [
        "def train_loop(data, optimizer, criterion_slots, criterion_intents, model):\n",
        "    model.train()\n",
        "    loss_array = []\n",
        "    for i,sample in enumerate(data):\n",
        "        optimizer.zero_grad() # Zeroing the gradient\n",
        "        slots, intent = model(sample['utterances'], sample['slots_len'])\n",
        "        loss_intent = criterion_intents(intent, sample['intents'])\n",
        "        loss_slot = criterion_slots(slots, sample['y_slots'])\n",
        "        loss = loss_intent+loss_slot # In joint training we sum the losses. \n",
        "        loss_array.append(loss.item())\n",
        "        loss.backward() # Compute the gradient, deleting the computational graph\n",
        "        optimizer.step() # Update the weights\n",
        "    return loss_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CAmEKzWSHe1"
      },
      "outputs": [],
      "source": [
        "def eval_loop(data, criterion_slots, criterion_intents, model, lang):\n",
        "    model.eval()\n",
        "    loss_array = []\n",
        "    \n",
        "    ref_intents = []\n",
        "    hyp_intents = []\n",
        "    \n",
        "    ref_slots = []\n",
        "    hyp_slots = []\n",
        "    #softmax = nn.Softmax(dim=1) # Use Softmax if you need the actual probability\n",
        "    with torch.no_grad(): # It used to avoid the creation of computational graph\n",
        "        total_slot_labels = [x for x in range (len(lang.id2slot))]\n",
        "        total_intent_labels = [v for k, v in lang.id2intent.items()]\n",
        "        cm_slot = np.zeros((len(total_slot_labels), len(total_slot_labels)))\n",
        "        cm_intent = np.zeros((len(total_intent_labels), len(total_intent_labels)))\n",
        "        for sample in data:\n",
        "            slots, intents = model(sample['utterances'], sample['slots_len'])\n",
        "            loss_intent = criterion_intents(intents, sample['intents'])\n",
        "            loss_slot = criterion_slots(slots, sample['y_slots'])\n",
        "            loss = loss_intent + loss_slot \n",
        "            loss_array.append(loss.item())\n",
        "            # Intent inference\n",
        "            # Get the highest probable class\n",
        "            out_intents = [lang.id2intent[x] \n",
        "                           for x in torch.argmax(intents, dim=1).tolist()] \n",
        "            gt_intents = [lang.id2intent[x] for x in sample['intents'].tolist()]\n",
        "            ref_intents.extend(gt_intents)\n",
        "            hyp_intents.extend(out_intents)\n",
        "            # Slot inference \n",
        "            output_slots = torch.argmax(slots, dim=1)\n",
        "\n",
        "            ref_int_labels = []\n",
        "            hyp_int_labels = []\n",
        "\n",
        "            for id_seq, seq in enumerate(output_slots):\n",
        "                length = sample['slots_len'].tolist()[id_seq]\n",
        "                utt_ids = sample['utterance'][id_seq][:length].tolist()\n",
        "                gt_ids = sample['y_slots'][id_seq].tolist()\n",
        "                gt_slots = [lang.id2slot[elem] for elem in gt_ids[:length]]\n",
        "                utterance = [lang.id2word[elem] for elem in utt_ids]\n",
        "                to_decode = seq[:length].tolist()\n",
        "                ref_slots.append([(utterance[id_el], elem) for id_el, elem in enumerate(gt_slots)])\n",
        "                tmp_seq = []\n",
        "                for id_el, elem in enumerate(to_decode):\n",
        "                  tmp_seq.append((utterance[id_el], lang.id2slot[elem]))\n",
        "                  ref_int_labels.extend(gt_ids[:length])\n",
        "                  hyp_int_labels.extend(to_decode)\n",
        "                    \n",
        "                hyp_slots.append(tmp_seq)\n",
        "\n",
        "        X = [lang.id2slot[x] for x in ref_int_labels]\n",
        "        Y = [lang.id2slot[x] for x in hyp_int_labels]\n",
        "        labels = [lang.id2slot[x] for x in total_slot_labels]\n",
        "\n",
        "        cm_slot += confusion_matrix(X, Y,labels = labels)\n",
        "        cm_intent += confusion_matrix(ref_intents, hyp_intents,labels = total_intent_labels)\n",
        "    \n",
        "    try:            \n",
        "        results = evaluate(ref_slots, hyp_slots)\n",
        "    except Exception as ex:\n",
        "        # Sometimes the model predics a class that is not in REF\n",
        "        results = None\n",
        "    \n",
        "    #results = sklearn.metrics.f1_score([lang.id2slot[x] for x in ref_int_labels], [lang.id2slot[x] for x in hyp_int_labels], average = 'weighted')\n",
        "    \n",
        "    report_intent = classification_report(ref_intents, hyp_intents, \n",
        "                                          zero_division=False, output_dict=True)\n",
        "    return results, report_intent, loss_array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00Xe6IXuCOMo"
      },
      "source": [
        "# Eval loop with confusion matrices"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modified evaluation loop to also keep track of confusion matrix information, in four lists:\n",
        "\n",
        "```\n",
        "\tref_intents = intent ground truth\n",
        "\thyp_intents = intent hypothesis from model\n",
        "\tref_slots = slots ground truth\n",
        "\thyp_slots = slots hypothesis from model\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "lIFL9YlWosLn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OMHU3P6CMP0"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "def eval_loop_wcm(data, criterion_slots, criterion_intents, model, lang):\n",
        "\tmodel.eval()\n",
        "\tloss_array = []\n",
        "\t\n",
        "\tref_intents = []\n",
        "\thyp_intents = []\n",
        "\t\n",
        "\tref_slots = []\n",
        "\thyp_slots = []\n",
        "\n",
        "\tslot_label_ids = [sl for sl in range (len(lang.id2slot))]\n",
        "\tintent_labels = [v for k, v in lang.id2intent.items()]\n",
        "\tconf_mat_slots = np.zeros((len(slot_label_ids), len(slot_label_ids)))\n",
        "\tconf_mat_intents = np.zeros((len(intent_labels), len(intent_labels)))\n",
        "\t\n",
        "\twith torch.no_grad(): # It used to avoid the creation of computational graph\n",
        "\t\tfor sample in data:\n",
        "\t\t\tslots, intents = model(sample['utterances'], sample['slots_len'])\n",
        "\n",
        "\t\t\tloss_intent = criterion_intents(intents, sample['intents'])\n",
        "\t\t\tloss_slot = criterion_slots(slots, sample['y_slots'])\n",
        "\t\t\tloss = loss_intent + loss_slot \n",
        "\t\t\tloss_array.append(loss.item())\n",
        "\t\t\t# Intent inference\n",
        "\t\t\t# Get the highest probable class\n",
        "\t\t\tout_intents = [lang.id2intent[x] for x in torch.argmax(intents, dim=1).tolist()] \n",
        "\t\t\tgt_intents = [lang.id2intent[x] for x in sample['intents'].tolist()]\n",
        "\t\t\tref_intents.extend(gt_intents)\n",
        "\t\t\thyp_intents.extend(out_intents)\n",
        "\t \n",
        "\t\t\tref_intent_labels = []\n",
        "\t\t\thyp_intent_labels = []\n",
        "\t\t\t\n",
        "\t\t\t# Slot inference \n",
        "\t\t\toutput_slots = torch.argmax(slots, dim=1)\n",
        "\t\t\tfor id_seq, seq in enumerate(output_slots):\n",
        "\t\t\t\tlength = sample['slots_len'].tolist()[id_seq]\n",
        "\t\t\t\tutt_ids = sample['utterance'][id_seq][:length].tolist()\n",
        "\t\t\t\tgt_ids = sample['y_slots'][id_seq].tolist()\n",
        "\t\t\t\tgt_slots = [lang.id2slot[elem] for elem in gt_ids[:length]]\n",
        "\t\t\t\tutterance = [lang.id2word[elem] for elem in utt_ids]\n",
        "\t\t\t\tto_decode = seq[:length].tolist()\n",
        "\t\t\t\tref_slots.append([(utterance[id_el], elem) for id_el, elem in enumerate(gt_slots)])\n",
        "\t\t\t\ttmp_seq = []\n",
        "\t\t\t\t\n",
        "\t\t\t\tfor id_el, elem in enumerate(to_decode):\n",
        "\t\t\t\t\ttmp_seq.append((utterance[id_el], lang.id2slot[elem]))\n",
        "\t\t\t\t\tref_intent_labels.extend(gt_ids[:length])\n",
        "\t\t\t\t\thyp_intent_labels.extend(to_decode)\n",
        "\t\n",
        "\t\t\t\thyp_slots.append(tmp_seq)\n",
        "\n",
        "\t\t\tX = [lang.id2slot[x] for x in ref_intent_labels]\n",
        "\t\t\tY = [lang.id2slot[x] for x in hyp_intent_labels]\n",
        "\t\t\tslot_labels = [lang.id2slot[x] for x in slot_label_ids]\n",
        "\n",
        "\t\t\tconf_mat_slots += confusion_matrix(X, Y,labels = slot_labels)\n",
        "\t\t\tconf_mat_intents += confusion_matrix(ref_intents, hyp_intents,labels = intent_labels)\n",
        "\n",
        "\ttry:         \n",
        "\t\tresults = evaluate(ref_slots, hyp_slots)\n",
        "\texcept Exception as ex:\n",
        "  \t# Sometimes the model predics a class that is not in REF\n",
        "\t\tprint(ex)\n",
        "\t\tref_s = set([x[1] for x in ref_slots])\n",
        "\t\thyp_s = set([x[1] for x in hyp_slots])\n",
        "\t\t\n",
        "\t\tprint(hyp_s.difference(ref_s))\n",
        "\t\tresults = []\n",
        "\t\t\n",
        "\treport_intent = classification_report(ref_intents, hyp_intents, zero_division=False, output_dict=True)\n",
        "\treturn results, report_intent, loss_array, conf_mat_slots, conf_mat_intents\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBc6mXUiyRzY"
      },
      "source": [
        "# Train and eval function for 5 iterations"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Auxiliary function to train the same model five times depending on the type passed. \n",
        "\n",
        "Also prints intermediate results, and not just mean and variance, in case colab stops the execution."
      ],
      "metadata": {
        "id": "rwalBEnCpjiS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lDf7pC2yXk6"
      },
      "outputs": [],
      "source": [
        "def train_5_times(model_type, train_loader, dev_loader, test_loader, lang): \n",
        "    hid_size = 200\n",
        "    emb_size = 300\n",
        "\n",
        "    lr = 0.0001 # learning rate\n",
        "    clip = 5 # Clip the gradient\n",
        "\n",
        "    out_slot = len(lang.slot2id)\n",
        "    out_int = len(lang.intent2id)\n",
        "    vocab_len = len(lang.word2id)\n",
        "\n",
        "    runs = 5\n",
        "    slot_f1s, intent_acc = [], []\n",
        "    for x in range(0, runs):\n",
        "        print(\"Run: \", x)\n",
        "        model = model_type(hid_size, out_slot, out_int, emb_size, vocab_len, pad_index=PAD_TOKEN)#.to(device)\n",
        "        model.apply(init_weights)\n",
        "\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "        criterion_slots = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)#.to(device)\n",
        "        criterion_intents = nn.CrossEntropyLoss()#.to(device)\n",
        "        \n",
        "        n_epochs = 200\n",
        "        losses_train = []\n",
        "        losses_dev = []\n",
        "        sampled_epochs = []\n",
        "        \n",
        "        early_stop = EarlyStopping(2) # Using patience = 2 to speed up training\n",
        "        \n",
        "        for x in tqdm(range(1,n_epochs)):\n",
        "            loss = train_loop(train_loader, optimizer, criterion_slots, criterion_intents, model)\n",
        "            if x % 5 == 0:\n",
        "                sampled_epochs.append(x)\n",
        "                losses_train.append(np.asarray(loss).mean())\n",
        "                results_dev, intent_res, loss_dev = eval_loop(dev_loader, criterion_slots, criterion_intents, model, lang)\n",
        "                losses_dev.append(np.asarray(loss_dev).mean())\n",
        "                f1 = results_dev['total']['f']\n",
        "\n",
        "                if not early_stop.update(f1): \n",
        "                    early_stop.reset()\n",
        "                    break\n",
        "\n",
        "\n",
        "        results_test, intent_test, _ = eval_loop(test_loader, criterion_slots, criterion_intents, model, lang)\n",
        "        intent_acc.append(intent_test['accuracy'])\n",
        "        slot_f1s.append(results_test['total']['f'])\n",
        "\n",
        "        print(\"Intermediate Slot F1: \", results_test['total']['f'])\n",
        "        print(\"Intermediate v: \", intent_test['accuracy'])\n",
        "\n",
        "    slot_f1s = np.asarray(slot_f1s)\n",
        "    intent_acc = np.asarray(intent_acc)\n",
        "    print('Slot F1', round(slot_f1s.mean(),3), '+-', round(slot_f1s.std(),3))\n",
        "    print('Intent Acc', round(intent_acc.mean(), 3), '+-', round(slot_f1s.std(), 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQmgc1gRCcOf"
      },
      "source": [
        "# Baseline Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2EQzByxRwNu"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "class ModelIAS(nn.Module):\n",
        "\n",
        "    def __init__(self, hid_size, out_slot, out_int, emb_size, vocab_len, n_layer=1, pad_index=0):\n",
        "        super(ModelIAS, self).__init__()\n",
        "        # hid_size = Hidden size\n",
        "        # out_slot = number of slots (output size for slot filling)\n",
        "        # out_int = number of intents (ouput size for intent class)\n",
        "        # emb_size = word embedding size\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_len, emb_size, padding_idx=pad_index)\n",
        "        \n",
        "        self.utt_encoder = nn.LSTM(emb_size, hid_size, n_layer, bidirectional=False)    \n",
        "        self.slot_out = nn.Linear(hid_size, out_slot)\n",
        "        self.intent_out = nn.Linear(hid_size, out_int)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        \n",
        "    def forward(self, utterance, seq_lengths):\n",
        "        # utterance.size() = batch_size X seq_len\n",
        "        utt_emb = self.embedding(utterance) # utt_emb.size() = batch_size X seq_len X emb_size\n",
        "        utt_emb = utt_emb.permute(1,0,2) # we need seq len first -> seq_len X batch_size X emb_size\n",
        "        \n",
        "        # pack_padded_sequence avoid computation over pad tokens reducing the computational cost\n",
        "        \n",
        "        packed_input = pack_padded_sequence(utt_emb, seq_lengths.cpu().numpy())\n",
        "        # Process the batch\n",
        "        packed_output, (last_hidden, cell) = self.utt_encoder(packed_input) \n",
        "        # Unpack the sequence\n",
        "        utt_encoded, input_sizes = pad_packed_sequence(packed_output)\n",
        "        # Get the last hidden state\n",
        "        last_hidden = last_hidden[-1,:,:]\n",
        "        # Compute slot logits\n",
        "        slots = self.slot_out(utt_encoded)\n",
        "        # Compute intent logits\n",
        "        intent = self.intent_out(last_hidden)\n",
        "        \n",
        "        # Slot size: seq_len, batch size, calsses \n",
        "        slots = slots.permute(1,2,0) # We need this for computing the loss\n",
        "        # Slot size: batch_size, classes, seq_len\n",
        "        return slots, intent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOjP6shURyoU"
      },
      "outputs": [],
      "source": [
        "def init_weights(mat):\n",
        "    for m in mat.modules():\n",
        "        if type(m) in [nn.GRU, nn.LSTM, nn.RNN]:\n",
        "            for name, param in m.named_parameters():\n",
        "                if 'weight_ih' in name:\n",
        "                    for idx in range(4):\n",
        "                        mul = param.shape[0]//4\n",
        "                        torch.nn.init.xavier_uniform_(param[idx*mul:(idx+1)*mul])\n",
        "                elif 'weight_hh' in name:\n",
        "                    for idx in range(4):\n",
        "                        mul = param.shape[0]//4\n",
        "                        torch.nn.init.orthogonal_(param[idx*mul:(idx+1)*mul])\n",
        "                elif 'bias' in name:\n",
        "                    param.data.fill_(0)\n",
        "        else:\n",
        "            if type(m) in [nn.Linear]:\n",
        "                torch.nn.init.uniform_(m.weight, -0.01, 0.01)\n",
        "                if m.bias != None:\n",
        "                    m.bias.data.fill_(0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdanddaVrMZU"
      },
      "source": [
        "### ATIS Training and Testing of Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyaWTM11R0_7"
      },
      "outputs": [],
      "source": [
        "hid_size = 200\n",
        "emb_size = 300\n",
        "\n",
        "lr = 0.0001 # learning rate\n",
        "clip = 5 # Clip the gradient\n",
        "\n",
        "out_slot_ATIS = len(lang_ATIS.slot2id)\n",
        "out_int_ATIS = len(lang_ATIS.intent2id)\n",
        "vocab_len_ATIS = len(lang_ATIS.word2id)\n",
        "\n",
        "model_ATIS = ModelIAS(hid_size, out_slot_ATIS, out_int_ATIS, emb_size, vocab_len_ATIS, pad_index=PAD_TOKEN)#.to(device)\n",
        "model_ATIS.apply(init_weights)\n",
        "\n",
        "optimizer = optim.Adam(model_ATIS.parameters(), lr=lr)\n",
        "criterion_slots = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
        "criterion_intents = nn.CrossEntropyLoss() # Because we do not have the pad token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "N6poQvVHSGBy",
        "outputId": "1a12eba0-7de6-4732-9b73-33b2afc82b26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|â         | 8/199 [01:07<26:55,  8.46s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-773907118383>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader_ATIS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion_slots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion_intents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_ATIS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0msampled_epochs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-9e5d919cd18f>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(data, optimizer, criterion_slots, criterion_intents, model)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_intent\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mloss_slot\u001b[0m \u001b[0;31m# In joint training we sum the losses.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mloss_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Compute the gradient, deleting the computational graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Update the weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss_array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "n_epochs = 200\n",
        "losses_train = []\n",
        "losses_dev = []\n",
        "sampled_epochs = []\n",
        "\n",
        "early_stop = EarlyStopping(3)\n",
        "\n",
        "for x in tqdm(range(1,n_epochs)):\n",
        "    loss = train_loop(train_loader_ATIS, optimizer, criterion_slots, criterion_intents, model_ATIS)\n",
        "    if x % 5 == 0:\n",
        "        sampled_epochs.append(x)\n",
        "        losses_train.append(np.asarray(loss).mean())\n",
        "        results_dev, intent_res, loss_dev = eval_loop(dev_loader_ATIS, criterion_slots, criterion_intents, model_ATIS, lang_ATIS)\n",
        "        losses_dev.append(np.asarray(loss_dev).mean())\n",
        "        f1 = results_dev['total']['f']\n",
        "        \n",
        "        if not early_stop.update(f1): \n",
        "            early_stop.reset()    \n",
        "            break\n",
        "\n",
        "#results_test, intent_test, _ = eval_loop(test_loader, criterion_slots, criterion_intents, model, lang)    \n",
        "results_test, intent_test, _, cm_slot, cm_int = eval_loop_wcm(test_loader_ATIS, criterion_slots, criterion_intents, model_ATIS, lang_ATIS)    \n",
        "print('Slot F1: ', results_test['total']['f'])\n",
        "print('Intent Accuracy:', intent_test['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UHEXtpvDLs4"
      },
      "outputs": [],
      "source": [
        "plot_losses(sampled_epochs, losses_train, losses_dev)\n",
        "\n",
        "plot_confusion_matrix(cm_int, label=\"Intents\")\n",
        "plot_confusion_matrix(cm_slot, label=\"Slots\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZs1s3CO0UMc"
      },
      "outputs": [],
      "source": [
        "train_5_times(ModelIAS, train_loader_ATIS, dev_loader_ATIS, test_loader_ATIS, lang_ATIS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lu8wl8_csigi"
      },
      "source": [
        "### SNIPS Training and Testing of Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vP-vjCc8soIb"
      },
      "outputs": [],
      "source": [
        "hid_size = 200\n",
        "emb_size = 300\n",
        "\n",
        "lr = 0.0001 # learning rate\n",
        "clip = 5 # Clip the gradient\n",
        "\n",
        "out_slot_SNIPS = len(lang_SNIPS.slot2id)\n",
        "out_int_SNIPS = len(lang_SNIPS.intent2id)\n",
        "vocab_len_SNIPS = len(lang_SNIPS.word2id)\n",
        "\n",
        "model_SNIPS = ModelIAS(hid_size, out_slot_SNIPS, out_int_SNIPS, emb_size, vocab_len_SNIPS, pad_index=PAD_TOKEN)#.to(device)\n",
        "model_SNIPS.apply(init_weights)\n",
        "\n",
        "optimizer = optim.Adam(model_SNIPS.parameters(), lr=lr)\n",
        "criterion_slots = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
        "criterion_intents = nn.CrossEntropyLoss() # Because we do not have the pad token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOhr803bs35l"
      },
      "outputs": [],
      "source": [
        "n_epochs = 200\n",
        "losses_train = []\n",
        "losses_dev = []\n",
        "sampled_epochs = []\n",
        "\n",
        "early_stop = EarlyStopping(3)\n",
        "\n",
        "for x in tqdm(range(1,n_epochs)):\n",
        "    loss = train_loop(train_loader_SNIPS, optimizer, criterion_slots, criterion_intents, model_SNIPS)\n",
        "    if x % 5 == 0:\n",
        "        sampled_epochs.append(x)\n",
        "        losses_train.append(np.asarray(loss).mean())\n",
        "        results_dev, intent_res, loss_dev = eval_loop(dev_loader_SNIPS, criterion_slots, criterion_intents, model_SNIPS, lang_SNIPS)\n",
        "        losses_dev.append(np.asarray(loss_dev).mean())\n",
        "        f1 = results_dev['total']['f']\n",
        "        \n",
        "        if not early_stop.update(f1): \n",
        "            early_stop.reset()    \n",
        "            break\n",
        "\n",
        "#results_test, intent_test, _ = eval_loop(test_loader, criterion_slots, criterion_intents, model, lang)    \n",
        "results_test, intent_test, _, cm_slot, cm_int = eval_loop_wcm(test_loader_SNIPS, criterion_slots, criterion_intents, model_SNIPS, lang_SNIPS)    \n",
        "print('Slot F1: ', results_test['total']['f'])\n",
        "print('Intent Accuracy:', intent_test['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puHSvxqjXsr8"
      },
      "outputs": [],
      "source": [
        "plot_losses(sampled_epochs, losses_train, losses_dev)\n",
        "\n",
        "plot_confusion_matrix(cm_int, label=\"Intents\")\n",
        "plot_confusion_matrix(cm_slot, label=\"Slots\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntamcJe85ZpC"
      },
      "outputs": [],
      "source": [
        "train_5_times(ModelIAS, train_loader_SNIPS, dev_loader_SNIPS, test_loader_SNIPS, lang_SNIPS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pwy702LoGq_E"
      },
      "source": [
        "# BiDirectional GRU model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjlhh2QPzmQR"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class BiDirectionalGRUModel(nn.Module):\n",
        "\n",
        "    def __init__(self, hid_size, out_slot, out_int, emb_size, vocab_len, n_layer=1, pad_index=0,  dropout=0.1):\n",
        "        super(BiDirectionalGRUModel, self).__init__()\n",
        "\n",
        "        self.hidden = None\n",
        "        self.dropout = dropout\n",
        "        \n",
        "        self.bidirectionality = True\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_len, emb_size, padding_idx=pad_index)\n",
        "        \n",
        "        self.gru_cell = nn.GRU(emb_size, hid_size, n_layer, bidirectional=self.bidirectionality)   \n",
        "        \n",
        "        slot_input_size = hid_size * 2 if self.bidirectionality else hid_size \n",
        "        self.slot_out = nn.Linear(slot_input_size, out_slot)\n",
        "        self.intent_out = nn.Linear(hid_size, out_int)\n",
        "        \n",
        "    def forward(self, utterance, seq_lengths):\n",
        "        # utterance.size() = batch_size X seq_len\n",
        "        utt_emb = self.embedding(utterance) # utt_emb.size() = batch_size X seq_len X emb_size\n",
        "        utt_emb = F.dropout(utt_emb, self.dropout)\n",
        "        utt_emb = utt_emb.permute(1,0,2) # we need seq len first -> seq_len X batch_size X emb_size\n",
        "        # Process the batch\n",
        "\n",
        "        packed_input = pack_padded_sequence(utt_emb, seq_lengths.cpu().numpy())\n",
        "\n",
        "        packed_output, self.hidden = self.gru_cell(packed_input) \n",
        "        \n",
        "        packed_output, input_sizes = pad_packed_sequence(packed_output)\n",
        "\n",
        "        packed_output = F.dropout(packed_output, self.dropout)\n",
        "        self.hidden = F.dropout(self.hidden, self.dropout)\n",
        "\n",
        "        last_hidden = self.hidden[-1,:,:]\n",
        "\n",
        "        # Compute slot logits\n",
        "        slots = self.slot_out(packed_output)\n",
        "        # Compute intent logits\n",
        "        intent = self.intent_out(last_hidden)\n",
        "        \n",
        "        # Slot size: seq_len, batch size, calsses \n",
        "        slots = slots.permute(1,2,0) # We need this for computing the loss\n",
        "        # Slot size: batch_size, classes, seq_len\n",
        "        return slots, intent"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The train loop for the GRU model is modified to use a different loss function (explained better in the report)"
      ],
      "metadata": {
        "id": "kSeU38_Np5NX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aty7cjBMCbgN"
      },
      "outputs": [],
      "source": [
        "def train_loop(data, optimizer, criterion_slots, criterion_intents, model):\n",
        "    model.train()\n",
        "    loss_array = []\n",
        "    for i,sample in enumerate(data):\n",
        "        optimizer.zero_grad() # Zeroing the gradient\n",
        "        slots, intent = model(sample['utterances'], sample['slots_len'])\n",
        "        loss_intent = criterion_intents(intent, sample['intents'])\n",
        "        loss_slot = criterion_slots(slots, sample['y_slots'])\n",
        "\n",
        "        # weighted loss from paper\n",
        "        weights, _ = torch.sort(F.softmax(torch.randn(2), dim=-1)) \n",
        "        loss = max(loss_intent, loss_slot) * max(weights[0], weights[1]) + min(loss_intent, loss_slot) * min(weights[0], weights[1])\n",
        "\n",
        "        loss_array.append(loss.item())\n",
        "        loss.backward() # Compute the gradient, deleting the computational graph\n",
        "        optimizer.step() # Update the weights\n",
        "    return loss_array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyIMFC9wu61y"
      },
      "source": [
        "### ATIS Bi-Directional Model Training and Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RknQLnkrsfT"
      },
      "outputs": [],
      "source": [
        "hid_size = 200\n",
        "emb_size = 300\n",
        "\n",
        "lr = 0.0001 # learning rate\n",
        "clip = 5 # Clip the gradient\n",
        "\n",
        "bi_model_ATIS = BiDirectionalGRUModel(hid_size, out_slot_ATIS, out_int_ATIS, emb_size, vocab_len_ATIS, pad_index=PAD_TOKEN)#.to(device)\n",
        "bi_model_ATIS.apply(init_weights)\n",
        "\n",
        "optimizer = optim.Adam(bi_model_ATIS.parameters(), lr=lr)\n",
        "criterion_slots = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
        "criterion_intents = nn.CrossEntropyLoss() # Because we do not have the pad token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rS5vdVlnCtyu"
      },
      "outputs": [],
      "source": [
        "n_epochs = 200\n",
        "\n",
        "losses_train = []\n",
        "losses_dev = []\n",
        "sampled_epochs = []\n",
        "\n",
        "early_stop = EarlyStopping(3)\n",
        "\n",
        "for x in tqdm(range(1,n_epochs)):\n",
        "    loss = train_loop(train_loader_ATIS, optimizer, criterion_slots, criterion_intents, bi_model_ATIS)\n",
        "    if x % 5 == 0:\n",
        "        sampled_epochs.append(x)\n",
        "        losses_train.append(np.asarray(loss).mean())\n",
        "        results_dev, intent_res, loss_dev = eval_loop(dev_loader_ATIS, criterion_slots, criterion_intents, bi_model_ATIS, lang_ATIS)\n",
        "        losses_dev.append(np.asarray(loss_dev).mean())\n",
        "        if results_dev != None: \n",
        "          f1 = results_dev['total']['f']\n",
        "\n",
        "          if not early_stop.update(f1): \n",
        "              early_stop.reset()\n",
        "              break\n",
        "        else:\n",
        "          print(\"Skipped eval at epoch: \", x) \n",
        "\n",
        "results_test, intent_test, _, cm_slot, cm_int = eval_loop_wcm(test_loader_ATIS, criterion_slots, criterion_intents, bi_model_ATIS, lang_ATIS)    \n",
        "print('Slot F1: ', results_test['total']['f'])\n",
        "print('Intent Accuracy:', intent_test['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvmrQoFpzmMo"
      },
      "outputs": [],
      "source": [
        "plot_losses(sampled_epochs, losses_train, losses_dev)\n",
        "\n",
        "plot_confusion_matrix(cm_int, label=\"Intents\")\n",
        "plot_confusion_matrix(cm_slot, label=\"Slots\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hc457f7A5gtK"
      },
      "outputs": [],
      "source": [
        "train_5_times(BiDirectionalGRUModel, train_loader_ATIS, dev_loader_ATIS, test_loader_ATIS, lang_ATIS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qv1zvjEMT5qz"
      },
      "source": [
        "Since training 5 iterations consecutively took too long, each training was executed separately, and mean and variance were calculated in the following way:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXlB2GyuSBta"
      },
      "outputs": [],
      "source": [
        "f1_results = [0.9306860485729481, 0.929100529100529, 0.9337114498404822,  0.9305949008498584,  0.9329322243850646]\n",
        "acc_results = [0.9518477043673013, 0.9596864501679732, 0.948488241881299, 0.9563269876819709, 0.9574468085106383]\n",
        "\n",
        "f1_mean = sum(f1_results) / len(f1_results)\n",
        "acc_mean = sum(acc_results) / len(acc_results)\n",
        "\n",
        "print(\"F1 Mean: \", f1_mean)\n",
        "print(\"F1 Var: \", round(sum([(f - f1_mean)**2 for f in f1_results]), 4))\n",
        "print(\"Acc Mean: \", acc_mean)\n",
        "print(\"Acc Var: \", round(sum((f - acc_mean)**2 for f in acc_results), 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpl7hyM4vstE"
      },
      "source": [
        "### SNIPS Bi-Directional Model Training and Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ere9i5htGGpa"
      },
      "outputs": [],
      "source": [
        "hid_size = 200\n",
        "emb_size = 300\n",
        "\n",
        "lr = 0.0001 # learning rate\n",
        "clip = 5 # Clip the gradient\n",
        "\n",
        "out_slot_SNIPS = len(lang_SNIPS.slot2id)\n",
        "out_int_SNIPS = len(lang_SNIPS.intent2id)\n",
        "vocab_len_SNIPS = len(lang_SNIPS.word2id)\n",
        "\n",
        "model_SNIPS = BiDirectionalGRUModel(hid_size, out_slot_SNIPS, out_int_SNIPS, emb_size, vocab_len_SNIPS, pad_index=PAD_TOKEN)#.to(device)\n",
        "model_SNIPS.apply(init_weights)\n",
        "\n",
        "optimizer = optim.Adam(model_SNIPS.parameters(), lr=lr)\n",
        "criterion_slots = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
        "criterion_intents = nn.CrossEntropyLoss() # Because we do not have the pad token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgN0TwIEvr84"
      },
      "outputs": [],
      "source": [
        "n_epochs = 200\n",
        "\n",
        "losses_train = []\n",
        "losses_dev = []\n",
        "sampled_epochs = []\n",
        "\n",
        "early_stop = EarlyStopping(3)\n",
        "\n",
        "for x in tqdm(range(1,n_epochs)):\n",
        "    loss = train_loop(train_loader_SNIPS, optimizer, criterion_slots, criterion_intents, model_SNIPS)\n",
        "    if x % 5 == 0:\n",
        "        sampled_epochs.append(x)\n",
        "        losses_train.append(np.asarray(loss).mean())\n",
        "        results_dev, intent_res, loss_dev = eval_loop(dev_loader_SNIPS, criterion_slots, criterion_intents, model_SNIPS, lang_SNIPS)\n",
        "        losses_dev.append(np.asarray(loss_dev).mean())\n",
        "        f1 = results_dev['total']['f']\n",
        "        \n",
        "        if not early_stop.update(f1): \n",
        "            early_stop.reset()    \n",
        "            break\n",
        "\n",
        "#results_test, intent_test, _ = eval_loop(test_loader, criterion_slots, criterion_intents, model, lang)    \n",
        "results_test, intent_test, _, cm_slot, cm_int = eval_loop_wcm(test_loader_SNIPS, criterion_slots, criterion_intents, model_SNIPS, lang_SNIPS)    \n",
        "print('Slot F1: ', results_test['total']['f'])\n",
        "print('Intent Accuracy:', intent_test['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDStYcNlwIQ-"
      },
      "outputs": [],
      "source": [
        "plot_losses(sampled_epochs, losses_train, losses_dev)\n",
        "\n",
        "plot_confusion_matrix(cm_int, label=\"Intents\")\n",
        "plot_confusion_matrix(cm_slot, label=\"Slots\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNv57HKNGJCd"
      },
      "outputs": [],
      "source": [
        "train_5_times(BiDirectionalGRUModel, train_loader_SNIPS, dev_loader_SNIPS, test_loader_SNIPS, lang_SNIPS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxTmeYKmM4m7"
      },
      "outputs": [],
      "source": [
        "f1_results = [0.8565229183618117, 0.8674698795180722, 0.850108225108225, 0.8686486486486485,0.8518619189997282]\n",
        "acc_results = [0.9628571428571429, 0.9571428571428572, 0.9685714285714285, 0.96, 0.9557142857142857]\n",
        "\n",
        "f1_mean = sum(f1_results) / len(f1_results)\n",
        "acc_mean = sum(acc_results) / len(acc_results)\n",
        "\n",
        "print(\"F1 Mean: \", f1_mean)\n",
        "print(\"F1 Var: \", round(sum([(f - f1_mean)**2 for f in f1_results]), 4))\n",
        "print(\"Acc Mean: \", acc_mean)\n",
        "print(\"Acc Var: \", round(sum((f - acc_mean)**2 for f in acc_results), 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qab9MP1QVwxq"
      },
      "source": [
        "# RNN Bidirectional Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wp1_or7JV1Rd"
      },
      "outputs": [],
      "source": [
        "class BiDirectionalRNNModel(nn.Module):\n",
        "\n",
        "    def __init__(self, hid_size, out_slot, out_int, emb_size, vocab_len, n_layer=2, pad_index=0):\n",
        "        super(BiDirectionalRNNModel, self).__init__()\n",
        "\n",
        "        self.hidden = None\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_len, emb_size, padding_idx=pad_index)\n",
        "        \n",
        "        self.rnn_layer = nn.RNN(emb_size, hid_size, n_layer, bidirectional=True)\n",
        "\n",
        "        self.slot_out = nn.Linear(hid_size*2, out_slot)\n",
        "        self.intent_out = nn.Linear(hid_size, out_int)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        \n",
        "    def forward(self, utterance, seq_lengths):\n",
        "        # utterance.size() = batch_size X seq_len\n",
        "        utt_emb = self.embedding(utterance) # utt_emb.size() = batch_size X seq_len X emb_size\n",
        "        utt_emb = self.dropout(utt_emb)\n",
        "        utt_emb = utt_emb.permute(1,0,2) # we need seq len first -> seq_len X batch_size X emb_size\n",
        "              \n",
        "        # Process the batch\n",
        "        encoded_output, self.hidden = self.rnn_layer(utt_emb) \n",
        "        last_hidden = self.hidden[-1,:,:]\n",
        "\n",
        "        encoded_output = self.dropout(encoded_output)\n",
        "        self.hidden = self.dropout(self.hidden)\n",
        "\n",
        "\n",
        "        # Compute slot logits\n",
        "        slots = self.slot_out(encoded_output)\n",
        "        # Compute intent logits\n",
        "        intent = self.intent_out(last_hidden)\n",
        "        \n",
        "        # Slot size: seq_len, batch size, calsses \n",
        "        slots = slots.permute(1,2,0) # We need this for computing the loss\n",
        "        # Slot size: batch_size, classes, seq_len\n",
        "        \n",
        "        return slots, intent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmBeTXIrlhB2"
      },
      "source": [
        "### BiDirectional RNN model (ATIS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEFYhbyKV3oL"
      },
      "outputs": [],
      "source": [
        "hid_size = 200\n",
        "emb_size = 300\n",
        "\n",
        "lr = 0.0001 # learning rate\n",
        "clip = 5 # Clip the gradient\n",
        "\n",
        "model_ATIS = BiDirectionalRNNModel(hid_size, out_slot_ATIS, out_int_ATIS, emb_size, vocab_len_ATIS, pad_index=PAD_TOKEN)#.to(device)\n",
        "model_ATIS.apply(init_weights)\n",
        "\n",
        "optimizer = optim.Adam(model_ATIS.parameters(), lr=lr)\n",
        "criterion_slots = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
        "criterion_intents = nn.CrossEntropyLoss() # Because we do not have the pad token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WsP4RjZkJBF"
      },
      "outputs": [],
      "source": [
        "n_epochs = 200\n",
        "\n",
        "losses_train = []\n",
        "losses_dev = []\n",
        "sampled_epochs = []\n",
        "\n",
        "early_stop = EarlyStopping(3)\n",
        "\n",
        "for x in tqdm(range(1,n_epochs)):\n",
        "    loss = train_loop(train_loader_ATIS, optimizer, criterion_slots, criterion_intents, model_ATIS)\n",
        "    if x % 5 == 0:\n",
        "        sampled_epochs.append(x)\n",
        "        losses_train.append(np.asarray(loss).mean())\n",
        "        results_dev, intent_res, loss_dev = eval_loop(dev_loader_ATIS, criterion_slots, criterion_intents, model_ATIS, lang_ATIS)\n",
        "        losses_dev.append(np.asarray(loss_dev).mean())\n",
        "        if results_dev != None: \n",
        "          f1 = results_dev['total']['f']\n",
        "\n",
        "          if not early_stop.update(f1): \n",
        "              early_stop.reset()\n",
        "              break\n",
        "        else:\n",
        "          print(\"Skipped eval at epoch: \", x) \n",
        "\n",
        "results_test, intent_test, _, cm_slot, cm_int = eval_loop_wcm(test_loader_ATIS, criterion_slots, criterion_intents, model_ATIS, lang_ATIS)    \n",
        "print('Slot F1: ', results_test['total']['f'])\n",
        "print('Intent Accuracy:', intent_test['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vfg3BQ17l8w1"
      },
      "outputs": [],
      "source": [
        "plot_losses(sampled_epochs, losses_train, losses_dev)\n",
        "\n",
        "plot_confusion_matrix(cm_int, label=\"Intents\")\n",
        "plot_confusion_matrix(cm_slot, label=\"Slots\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dprtluX5ip3"
      },
      "outputs": [],
      "source": [
        "train_5_times(BiDirectionalRNNModel, train_loader_ATIS, dev_loader_ATIS, test_loader_ATIS, lang_ATIS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsN97QcIlox-"
      },
      "source": [
        "### BiDirectional RNN model SNIPS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKRayLrblvlf"
      },
      "outputs": [],
      "source": [
        "hid_size = 200\n",
        "emb_size = 300\n",
        "\n",
        "lr = 0.0001 # learning rate\n",
        "clip = 5 # Clip the gradient\n",
        "\n",
        "out_slot_SNIPS = len(lang_SNIPS.slot2id)\n",
        "out_int_SNIPS = len(lang_SNIPS.intent2id)\n",
        "vocab_len_SNIPS = len(lang_SNIPS.word2id)\n",
        "\n",
        "model_SNIPS = BiDirectionalRNNModel(hid_size, out_slot_SNIPS, out_int_SNIPS, emb_size, vocab_len_SNIPS, pad_index=PAD_TOKEN)#.to(device)\n",
        "model_SNIPS.apply(init_weights)\n",
        "\n",
        "optimizer = optim.Adam(model_SNIPS.parameters(), lr=lr)\n",
        "criterion_slots = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
        "criterion_intents = nn.CrossEntropyLoss() # Because we do not have the pad token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXa8dNDDl1kX"
      },
      "outputs": [],
      "source": [
        "n_epochs = 200\n",
        "\n",
        "losses_train = []\n",
        "losses_dev = []\n",
        "sampled_epochs = []\n",
        "\n",
        "early_stop = EarlyStopping(2)\n",
        "\n",
        "for x in tqdm(range(1,n_epochs)):\n",
        "    loss = train_loop(train_loader_SNIPS, optimizer, criterion_slots, criterion_intents, model_SNIPS)\n",
        "    if x % 5 == 0:\n",
        "        sampled_epochs.append(x)\n",
        "        losses_train.append(np.asarray(loss).mean())\n",
        "        results_dev, intent_res, loss_dev = eval_loop(dev_loader_SNIPS, criterion_slots, criterion_intents, model_SNIPS, lang_SNIPS)\n",
        "        losses_dev.append(np.asarray(loss_dev).mean())\n",
        "        f1 = results_dev['total']['f']\n",
        "        \n",
        "        if not early_stop.update(f1): \n",
        "            early_stop.reset()    \n",
        "            break\n",
        "\n",
        "#results_test, intent_test, _ = eval_loop(test_loader, criterion_slots, criterion_intents, model, lang)    \n",
        "results_test, intent_test, _, cm_slot, cm_int = eval_loop_wcm(test_loader_SNIPS, criterion_slots, criterion_intents, model_SNIPS, lang_SNIPS)    \n",
        "print('Slot F1: ', results_test['total']['f'])\n",
        "print('Intent Accuracy:', intent_test['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBlfqFptl8Ab"
      },
      "outputs": [],
      "source": [
        "plot_losses(sampled_epochs, losses_train, losses_dev)\n",
        "\n",
        "plot_confusion_matrix(cm_int, label=\"Intents\")\n",
        "plot_confusion_matrix(cm_slot, label=\"Slots\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfTCpmwy2Xej"
      },
      "outputs": [],
      "source": [
        "train_5_times(BiDirectionalRNNModel, train_loader_SNIPS, dev_loader_SNIPS, test_loader_SNIPS, lang_SNIPS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-DP9IpEMvUy"
      },
      "outputs": [],
      "source": [
        "f1_results = [0.8394062078272604, 0.8304252998909487, 0.8709413369713506,  0.8570650694255376, 0.8687807545106616]\n",
        "acc_results = [0.9528571428571428, 0.9671428571428572, 0.9657142857142857, 0.9671428571428572, 0.9685714285714285]\n",
        "\n",
        "f1_mean = sum(f1_results) / len(f1_results)\n",
        "acc_mean = sum(acc_results) / len(acc_results)\n",
        "\n",
        "print(\"F1 Mean: \", f1_mean)\n",
        "print(\"F1 Var: \", round(sum([(f - f1_mean)**2 for f in f1_results]), 4))\n",
        "print(\"Acc Mean: \", acc_mean)\n",
        "print(\"Acc Var: \", round(sum((f - acc_mean)**2 for f in acc_results), 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz1paFOe2O5P"
      },
      "source": [
        "# EncoderDecoderModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwLFur_P9QKn"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "import torch.nn.functional as F\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EncoderDecoderModel is the  simpler version of the two models implemented following the paper `A Bi-model based RNN Semantic Frame Parsing Model for Intent Detection and Slot Filling.`. \n",
        "The forward pass for this architecture is very similar to the baseline and, implemented this way, it works similarly to appending more recursive layers to the model (`n_layer=2`)."
      ],
      "metadata": {
        "id": "eec8K-r4qWzJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbjdhIQ52SEr"
      },
      "outputs": [],
      "source": [
        "class EncoderDecoderModel(nn.Module):\n",
        "\n",
        "    def __init__(self, hid_size, out_slot, out_int, emb_size, vocab_len, n_layer=1, pad_index=0, dropout=0.1):\n",
        "        super(EncoderDecoderModel, self).__init__()\n",
        "\n",
        "        self.hidden = None\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Encoder\n",
        "        self.embedding = nn.Embedding(vocab_len, emb_size, padding_idx=pad_index)\n",
        "        self.enc_gru = nn.GRU(emb_size, hid_size, n_layer, bidirectional=False)  \n",
        "                        \n",
        "        # Decoder Intent\n",
        "        self.dec_gru_int = nn.GRU(hid_size, hid_size, n_layer, bidirectional=False)\n",
        "        self.intent_out = nn.Linear(hid_size, out_int)\n",
        "\n",
        "        # Decoder Slot\n",
        "        self.dec_gru_slot = nn.GRU(hid_size, hid_size, n_layer, bidirectional=False)\n",
        "        self.slot_out = nn.Linear(hid_size, out_slot)\n",
        "\n",
        "        \n",
        "    def forward(self, utterance, seq_lengths):\n",
        "        embedded_utt = self.embedding(utterance) \n",
        "        embedded_utt = self.dropout(embedded_utt)\n",
        "        embedded_utt = embedded_utt.permute(1,0,2) \n",
        "\n",
        "        # Encoding\n",
        "        embedded_utt_packed = pack_padded_sequence(embedded_utt, seq_lengths.cpu().numpy())\n",
        "        packed_output, self.hidden = self.enc_gru(embedded_utt_packed) \n",
        "        unpacked_output, input_sizes = pad_packed_sequence(packed_output)\n",
        "\n",
        "        last_hidden = self.hidden[-1,:,:]\n",
        "        \n",
        "        # Decoding INTENT\n",
        "        out_int, self.hidden = self.dec_gru_int(last_hidden) \n",
        "        out_int = self.dropout(out_int)\n",
        "        intent = self.intent_out(out_int)\n",
        "\n",
        "        # Decoding SLOTS\n",
        "        out_slot, self.hidden = self.dec_gru_slot(unpacked_output) \n",
        "        slots = self.slot_out(out_slot)\n",
        "        slots = self.dropout(slots)\n",
        "        slots = slots.permute(1,2,0) \n",
        "\n",
        "        return slots, intent\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EncoderDecoderModelTest is the more resembling model to the one in the paper. The forward pass iterates over all tokens in the sentence, so training is extremely slow."
      ],
      "metadata": {
        "id": "HiIKJ3a1rGaA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkRXt8OBGtzg"
      },
      "outputs": [],
      "source": [
        "class EncoderDecoderModelTest(nn.Module):\n",
        "\n",
        "    def __init__(self, hid_size, out_slot, out_int, emb_size, vocab_len, n_layer=1, pad_index=0, dropout=0.1):\n",
        "        super(EncoderDecoderModelTest, self).__init__()\n",
        "\n",
        "        self.hid_size = hid_size\n",
        "        self.hidden = None\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_len, emb_size, padding_idx=pad_index)\n",
        "\n",
        "        # Encoder Intent\n",
        "        self.enc_gru_int = nn.GRU(emb_size, hid_size, n_layer, bidirectional=False)  \n",
        "\n",
        "        # Encoder Slots\n",
        "        self.enc_gru_slot = nn.GRU(emb_size, hid_size, n_layer, bidirectional=False)  \n",
        "\n",
        "        # Decoder Intent\n",
        "        self.dec_gru_int = nn.GRU(hid_size*2, hid_size, n_layer, bidirectional=False)\n",
        "        self.intent_out = nn.Linear(hid_size, out_int)\n",
        "\n",
        "        # Decoder Slot\n",
        "        self.dec_gru_slot = nn.GRU(hid_size*3, hid_size, n_layer, bidirectional=False)\n",
        "        self.slot_out = nn.Linear(hid_size, out_slot)\n",
        "\n",
        "        \n",
        "    def forward(self, utterance, seq_lengths):\n",
        "        embedded_utt = self.embedding(utterance) \n",
        "        embedded_utt = embedded_utt.permute(1,0,2) \n",
        "\n",
        "        # Encoding INTENT\n",
        "        hi, hidden = self.enc_gru_int(embedded_utt) \n",
        "        hidden = hidden[-1,:,:]\n",
        "\n",
        "        # Encoding SLOTS\n",
        "        hs, _ = self.enc_gru_slot(embedded_utt) \n",
        "\n",
        "        # Decoding SLOTS\n",
        "        unpacked_output = torch.cat((hs, hi), dim=-1)\n",
        "        out = torch.zeros(unpacked_output.size(1), 1, self.hid_size)\n",
        "        hidden_state = torch.zeros(1, 1, self.hid_size)\n",
        "\n",
        "        all_out = []\n",
        "        length = unpacked_output.size(0)\n",
        "        for i in range(length):\n",
        "            inp = torch.cat((unpacked_output[i].unsqueeze(1), out), dim=-1)\n",
        "            out, hidden_state = self.dec_gru_slot(inp, hidden_state)\n",
        "            all_out.append(out)\n",
        "        out_slot = torch.cat(all_out, dim=1)\n",
        "        out_slot = out_slot.permute(1,0,2) # seq_len x batch_size x feature_size\n",
        "\n",
        "        slots = self.slot_out(out_slot)\n",
        "        slots = self.dropout(slots)\n",
        "\n",
        "        slots = slots.permute(1,2,0) \n",
        "        slots = F.log_softmax(slots, dim=1)\n",
        "\n",
        "        # Decoding INTENT\n",
        "        batch = hi.size()[1]\n",
        "        dec_int_input = torch.cat((hi, hs), dim=-1)\n",
        "        \n",
        "        out_int, self.hidden = self.dec_gru_int(dec_int_input) \n",
        "        #out_int, self.hidden = self.dec_gru_int(hidden) \n",
        "        out_int = self.dropout(out_int)\n",
        "        out_int = out_int.permute(1,0,2) \n",
        "\n",
        "        index = torch.arange(batch).long()\n",
        "        state = out_int[index, seq_lengths-1, :]\n",
        "        \n",
        "        intent = self.intent_out(state.squeeze())        \n",
        "        #intent = self.intent_out(out_int)\n",
        "\n",
        "        return slots, intent\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibf3XvIKiRgJ"
      },
      "source": [
        "### EncoderDecoder model training and testing with ATIS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFVTO-TJiuN5"
      },
      "outputs": [],
      "source": [
        "hid_size = 200\n",
        "emb_size = 300\n",
        "\n",
        "lr = 0.0001 # learning rate\n",
        "clip = 5 # Clip the gradient\n",
        "\n",
        "out_slot_ATIS = len(lang_ATIS.slot2id)\n",
        "out_int_ATIS = len(lang_ATIS.intent2id)\n",
        "vocab_len_ATIS = len(lang_ATIS.word2id)\n",
        "\n",
        "model_ATIS = EncoderDecoderModelTest(hid_size, out_slot_ATIS, out_int_ATIS, emb_size, vocab_len_ATIS, pad_index=PAD_TOKEN)#.to(device)\n",
        "model_ATIS.apply(init_weights)\n",
        "\n",
        "optimizer = optim.Adam(model_ATIS.parameters(), lr=lr)\n",
        "criterion_slots = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
        "criterion_intents = nn.CrossEntropyLoss() # Because we do not have the pad token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TDXqc3a9Nea"
      },
      "outputs": [],
      "source": [
        "n_epochs = 100\n",
        "losses_train = []\n",
        "losses_dev = []\n",
        "sampled_epochs = []\n",
        "\n",
        "early_stop = EarlyStopping(20)\n",
        "\n",
        "for x in tqdm(range(1,n_epochs)):\n",
        "    loss = train_loop(train_loader_ATIS, optimizer, criterion_slots, criterion_intents, model_ATIS)\n",
        "    if x % 5 == 0:\n",
        "        sampled_epochs.append(x)\n",
        "        losses_train.append(np.asarray(loss).mean())\n",
        "        results_dev, intent_res, loss_dev = eval_loop(dev_loader_ATIS, criterion_slots, criterion_intents, model_ATIS, lang_ATIS)\n",
        "        losses_dev.append(np.asarray(loss_dev).mean())\n",
        "        f1 = results_dev['total']['f']\n",
        "        \n",
        "        if not early_stop.update(f1): \n",
        "            early_stop.reset()    \n",
        "            break\n",
        "\n",
        "#results_test, intent_test, _ = eval_loop(test_loader, criterion_slots, criterion_intents, model, lang)    \n",
        "results_test, intent_test, _, cm_slot, cm_int = eval_loop_wcm(test_loader_ATIS, criterion_slots, criterion_intents, model_ATIS, lang_ATIS)    \n",
        "print('Slot F1: ', results_test['total']['f'])\n",
        "print('Intent Accuracy:', intent_test['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gy4ZtevJQinl"
      },
      "outputs": [],
      "source": [
        "plot_losses(sampled_epochs, losses_train, losses_dev)\n",
        "\n",
        "plot_confusion_matrix(cm_int, label=\"Intents\")\n",
        "plot_confusion_matrix(cm_slot, label=\"Slots\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YK_sUa-8fXi6"
      },
      "outputs": [],
      "source": [
        "train_5_times(EncoderDecoderModel, train_loader_ATIS, dev_loader_ATIS, test_loader_ATIS, lang_ATIS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CVZ9mq8icGY"
      },
      "source": [
        "### EncoderDecoder model training and testing with SNIPS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbWKerAdjC2c"
      },
      "outputs": [],
      "source": [
        "hid_size = 200\n",
        "emb_size = 300\n",
        "\n",
        "lr = 0.0001 # learning rate\n",
        "clip = 5 # Clip the gradient\n",
        "\n",
        "out_slot_SNIPS = len(lang_SNIPS.slot2id)\n",
        "out_int_SNIPS = len(lang_SNIPS.intent2id)\n",
        "vocab_len_SNIPS = len(lang_SNIPS.word2id)\n",
        "\n",
        "model_SNIPS = EncoderDecoderModelTest(hid_size, out_slot_SNIPS, out_int_SNIPS, emb_size, vocab_len_SNIPS, pad_index=PAD_TOKEN)#.to(device)\n",
        "model_SNIPS.apply(init_weights)\n",
        "\n",
        "optimizer = optim.Adam(model_SNIPS.parameters(), lr=lr)\n",
        "criterion_slots = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
        "criterion_intents = nn.CrossEntropyLoss() # Because we do not have the pad token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YA8EcvpxihYR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f6b5741-9718-4ce5-bb22-138362eea9de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|ââââââââââ| 99/99 [4:57:07<00:00, 180.07s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Slot F1:  0.8157539789587267\n",
            "Intent Accuracy: 0.9657142857142857\n"
          ]
        }
      ],
      "source": [
        "n_epochs = 100\n",
        "losses_train = []\n",
        "losses_dev = []\n",
        "sampled_epochs = []\n",
        "\n",
        "early_stop = EarlyStopping(10)\n",
        "\n",
        "for x in tqdm(range(1,n_epochs)):\n",
        "    loss = train_loop(train_loader_SNIPS, optimizer, criterion_slots, criterion_intents, model_SNIPS)\n",
        "    if x % 5 == 0:\n",
        "        sampled_epochs.append(x)\n",
        "        losses_train.append(np.asarray(loss).mean())\n",
        "        results_dev, intent_res, loss_dev = eval_loop(dev_loader_SNIPS, criterion_slots, criterion_intents, model_SNIPS, lang_SNIPS)\n",
        "        losses_dev.append(np.asarray(loss_dev).mean())\n",
        "        f1 = results_dev['total']['f']\n",
        "        \n",
        "        if not early_stop.update(f1): \n",
        "            early_stop.reset()    \n",
        "            break\n",
        "\n",
        "#results_test, intent_test, _ = eval_loop(test_loader, criterion_slots, criterion_intents, model, lang)    \n",
        "results_test, intent_test, _, cm_slot, cm_int = eval_loop_wcm(test_loader_SNIPS, criterion_slots, criterion_intents, model_SNIPS, lang_SNIPS)    \n",
        "print('Slot F1: ', results_test['total']['f'])\n",
        "print('Intent Accuracy:', intent_test['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Od3yHYLRijHv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "112d7c59-b07f-4654-da45-312c392c3ae0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plot_losses(sampled_epochs, losses_train, losses_dev)\n",
        "\n",
        "plot_confusion_matrix(cm_int, label=\"Intents\")\n",
        "plot_confusion_matrix(cm_slot, label=\"Slots\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTHkE3c-e8AQ"
      },
      "outputs": [],
      "source": [
        "train_5_times(EncoderDecoderModel, train_loader_SNIPS, dev_loader_SNIPS, test_loader_SNIPS, lang_SNIPS)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These results are averaged from the training of the simpler model (EncoderDecoderModel, not EncoderDecoderModelTest), as training for the more complex architecture took over two  hours for 100 epochs"
      ],
      "metadata": {
        "id": "fljUu37_4bsC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1i3NDovKMS5"
      },
      "outputs": [],
      "source": [
        "f1_results = [0.7960064759848893, 0.805697393173878, 0.7678476996298254,  0.7956873315363883, 0.7765089722675367]\n",
        "acc_results = [0.97, 0.9671428571428572, 0.9657142857142857, 0.9671428571428572, 0.9642857142857143]\n",
        "\n",
        "f1_mean = sum(f1_results) / len(f1_results)\n",
        "acc_mean = sum(acc_results) / len(acc_results)\n",
        "\n",
        "print(\"F1 Mean: \", f1_mean)\n",
        "print(\"F1 Var: \", round(sum([(f - f1_mean)**2 for f in f1_results]), 4))\n",
        "print(\"Acc Mean: \", acc_mean)\n",
        "print(\"Acc Var: \", round(sum((f - acc_mean)**2 for f in acc_results), 6))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "WHEyWj7DCx_c",
        "zCO6OoUOCsc3",
        "f0OpLuhznjpR",
        "0Gqu80ZnSfeg",
        "BfxYB_yRCQj2",
        "IO83hCbAKGiO",
        "CFUz-wL9-sxi",
        "_sfLELAWR-2j",
        "00Xe6IXuCOMo",
        "DQmgc1gRCcOf",
        "Lu8wl8_csigi",
        "Pwy702LoGq_E",
        "dyIMFC9wu61y",
        "hpl7hyM4vstE",
        "Qab9MP1QVwxq",
        "WmBeTXIrlhB2",
        "wsN97QcIlox-"
      ],
      "name": "NLU.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}